{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125a28fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msubplots\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_subplots\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fraud Detection - Exploratory Data Analysis\n",
    "===========================================\n",
    "\n",
    "This notebook provides comprehensive exploratory data analysis \n",
    "for the fraud detection dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"üìä Fraud Detection - Exploratory Data Analysis\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c13ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded from: ../data/raw/credit_card_transaction_train.csv\n",
      "üìà Dataset Overview:\n",
      "   Shape: (1296675, 24)\n",
      "   Fraud Rate: 0.58%\n",
      "   Date Range: 2019-01-01 00:00:18 to 2020-06-21 12:13:37\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Overview Data\n",
    "def load_fraud_data():\n",
    "    \"\"\"Load fraud detection dataset\"\"\"\n",
    "    try:\n",
    "        # Try to load from multiple possible locations\n",
    "        possible_paths = [\n",
    "            '../data/raw/credit_card_transaction_train.csv',\n",
    "            '../data/processed/train.parquet',\n",
    "            '../data/raw/fraud_data.csv'\n",
    "        ]\n",
    "        \n",
    "        for path in possible_paths:\n",
    "            try:\n",
    "                if path.endswith('.parquet'):\n",
    "                    df = pd.read_parquet(path)\n",
    "                else:\n",
    "                    df = pd.read_csv(path)\n",
    "                print(f\"‚úÖ Data loaded from: {path}\")\n",
    "                return df\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        \n",
    "        # If no file found, generate sample data\n",
    "        print(\"‚ö†Ô∏è No data files found. Generating sample data...\")\n",
    "        return generate_sample_data()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return generate_sample_data()\n",
    "\n",
    "def generate_sample_data(n_samples=5000):\n",
    "    \"\"\"Generate sample fraud detection data for analysis\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Basic transaction data\n",
    "    data = {\n",
    "        'trans_date_trans_time': pd.date_range('2023-01-01', periods=n_samples, freq='H'),\n",
    "        'cc_num': [f\"{np.random.randint(1000, 9999)}{np.random.randint(1000, 9999)}{np.random.randint(1000, 9999)}{np.random.randint(1000, 9999)}\" for _ in range(n_samples)],\n",
    "        'merchant': [f\"merchant_{np.random.randint(1, 1000)}\" for _ in range(n_samples)],\n",
    "        'category': np.random.choice(['grocery_pos', 'gas_transport', 'misc_net', 'grocery_net', 'entertainment', 'misc_pos'], n_samples),\n",
    "        'amt': np.random.lognormal(3, 1, n_samples),  # Log-normal distribution for amounts\n",
    "        'first': np.random.choice(['John', 'Jane', 'Michael', 'Sarah', 'David', 'Lisa'], n_samples),\n",
    "        'last': np.random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia'], n_samples),\n",
    "        'gender': np.random.choice(['M', 'F'], n_samples),\n",
    "        'street': [f\"{np.random.randint(1, 9999)} {np.random.choice(['Main', 'Oak', 'Pine', 'Elm'])} St\" for _ in range(n_samples)],\n",
    "        'city': [f\"City_{np.random.randint(1, 100)}\" for _ in range(n_samples)],\n",
    "        'state': np.random.choice(['CA', 'TX', 'FL', 'NY', 'PA', 'IL', 'OH', 'GA', 'NC', 'MI'], n_samples),\n",
    "        'zip': [f\"{np.random.randint(10000, 99999)}\" for _ in range(n_samples)],\n",
    "        'lat': np.random.uniform(25, 49, n_samples),\n",
    "        'long': np.random.uniform(-125, -66, n_samples),\n",
    "        'city_pop': np.random.randint(1000, 500000, n_samples),\n",
    "        'job': np.random.choice(['Engineer', 'Teacher', 'Doctor', 'Lawyer', 'Artist', 'Manager'], n_samples),\n",
    "        'dob': [f\"{np.random.randint(1950, 2000)}-{np.random.randint(1, 12):02d}-{np.random.randint(1, 28):02d}\" for _ in range(n_samples)],\n",
    "        'merch_lat': np.random.uniform(25, 49, n_samples),\n",
    "        'merch_long': np.random.uniform(-125, -66, n_samples),\n",
    "        'merch_zipcode': [f\"{np.random.randint(10000, 99999)}\" for _ in range(n_samples)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate fraud labels with realistic patterns\n",
    "    fraud_probability = (\n",
    "        (df['amt'] > df['amt'].quantile(0.95)) * 0.3 +  # High amounts\n",
    "        (np.abs(df['lat'] - df['merch_lat']) > 5) * 0.2 +  # Far from merchant\n",
    "        (pd.to_datetime(df['trans_date_trans_time']).dt.hour < 6) * 0.2 +  # Late night\n",
    "        np.random.random(n_samples) * 0.3  # Random component\n",
    "    )\n",
    "    \n",
    "    df['is_fraud'] = (fraud_probability > 0.6).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_fraud_data()\n",
    "\n",
    "print(\"üìà Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Fraud Rate: {df['is_fraud'].mean():.2%}\")\n",
    "print(f\"   Date Range: {df['trans_date_trans_time'].min()} to {df['trans_date_trans_time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5423b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Basic Dataset Information\n",
      "----------------------------------------\n",
      "Rows: 1,296,675\n",
      "Columns: 24\n",
      "Memory Usage: 1044.88 MB\n",
      "\n",
      "üìã Data Types:\n",
      "   object: 12 columns\n",
      "   int64: 6 columns\n",
      "   float64: 6 columns\n",
      "\n",
      "‚ö†Ô∏è Missing Values:\n",
      "               Missing Count  Percentage\n",
      "merch_zipcode         195973       15.11\n",
      "\n",
      "üéØ Target Variable (is_fraud):\n",
      "   Legitimate: 1,289,169 (99.42%)\n",
      "   Fraud: 7,506 (0.58%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Basic Data Information\n",
    "def display_basic_info(df):\n",
    "    \"\"\"Display basic information about the dataset\"\"\"\n",
    "    \n",
    "    print(\"üîç Basic Dataset Information\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Dataset dimensions\n",
    "    print(f\"Rows: {df.shape[0]:,}\")\n",
    "    print(f\"Columns: {df.shape[1]}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    \n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Missing Values:\")\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing Count': missing_values[missing_values > 0],\n",
    "            'Percentage': missing_percent[missing_values > 0]\n",
    "        }).round(2)\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values detected\")\n",
    "    \n",
    "    # Target variable distribution\n",
    "    print(\"\\nüéØ Target Variable (is_fraud):\")\n",
    "    fraud_counts = df['is_fraud'].value_counts()\n",
    "    fraud_pct = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in fraud_counts.items():\n",
    "        label = \"Fraud\" if value == 1 else \"Legitimate\"\n",
    "        print(f\"   {label}: {count:,} ({fraud_pct[value]:.2f}%)\")\n",
    "\n",
    "display_basic_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166aaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Transaction Amount Analysis\n",
      "----------------------------------------\n",
      "üìä Amount Statistics:\n",
      "   Count: $1296675.00\n",
      "   Mean: $70.35\n",
      "   Std: $160.32\n",
      "   Min: $1.00\n",
      "   25%: $9.65\n",
      "   50%: $47.52\n",
      "   75%: $83.14\n",
      "   Max: $28948.90\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_subplots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Median legitimate amount: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlegitimate_amounts.median()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Median fraud amount: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfraud_amounts.median()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43manalyze_transaction_amounts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36manalyze_transaction_amounts\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstat.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m fig = \u001b[43mmake_subplots\u001b[49m(\n\u001b[32m     16\u001b[39m     rows=\u001b[32m2\u001b[39m, cols=\u001b[32m2\u001b[39m,\n\u001b[32m     17\u001b[39m     subplot_titles=(\n\u001b[32m     18\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAmount Distribution (All Transactions)\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAmount Distribution by Fraud Status\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     20\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mBox Plot by Fraud Status\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     21\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAmount vs Fraud Rate by Bins\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m     ),\n\u001b[32m     23\u001b[39m     specs=[[{\u001b[33m\"\u001b[39m\u001b[33msecondary_y\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}, {\u001b[33m\"\u001b[39m\u001b[33msecondary_y\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}],\n\u001b[32m     24\u001b[39m            [{\u001b[33m\"\u001b[39m\u001b[33msecondary_y\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}, {\u001b[33m\"\u001b[39m\u001b[33msecondary_y\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}]]\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Overall distribution\u001b[39;00m\n\u001b[32m     28\u001b[39m fig.add_trace(\n\u001b[32m     29\u001b[39m     go.Histogram(x=df[\u001b[33m'\u001b[39m\u001b[33mamt\u001b[39m\u001b[33m'\u001b[39m], nbinsx=\u001b[32m50\u001b[39m, name=\u001b[33m'\u001b[39m\u001b[33mAll Transactions\u001b[39m\u001b[33m'\u001b[39m, opacity=\u001b[32m0.7\u001b[39m),\n\u001b[32m     30\u001b[39m     row=\u001b[32m1\u001b[39m, col=\u001b[32m1\u001b[39m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'make_subplots' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 4: Transaction Amount Analysis\n",
    "def analyze_transaction_amounts(df):\n",
    "    \"\"\"Comprehensive analysis of transaction amounts\"\"\"\n",
    "    \n",
    "    print(\"üí∞ Transaction Amount Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Basic statistics\n",
    "    amount_stats = df['amt'].describe()\n",
    "    print(\"üìä Amount Statistics:\")\n",
    "    for stat, value in amount_stats.items():\n",
    "        print(f\"   {stat.title()}: ${value:.2f}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Amount Distribution (All Transactions)',\n",
    "            'Amount Distribution by Fraud Status',\n",
    "            'Box Plot by Fraud Status',\n",
    "            'Amount vs Fraud Rate by Bins'\n",
    "        ),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Overall distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df['amt'], nbinsx=50, name='All Transactions', opacity=0.7),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Distribution by fraud status\n",
    "    legitimate = df[df['is_fraud'] == 0]['amt']\n",
    "    fraud = df[df['is_fraud'] == 1]['amt']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=legitimate, nbinsx=30, name='Legitimate', opacity=0.7),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=fraud, nbinsx=30, name='Fraud', opacity=0.7),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Box plots\n",
    "    fig.add_trace(\n",
    "        go.Box(y=legitimate, name='Legitimate', boxpoints='outliers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Box(y=fraud, name='Fraud', boxpoints='outliers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Fraud rate by amount bins\n",
    "    df['amount_bin'] = pd.cut(df['amt'], bins=20, labels=False)\n",
    "    fraud_by_bin = df.groupby('amount_bin')['is_fraud'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    fraud_by_bin['fraud_rate'] = fraud_by_bin['mean']\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=fraud_by_bin['amount_bin'], y=fraud_by_bin['fraud_rate'], \n",
    "                  mode='lines+markers', name='Fraud Rate'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Transaction Amount Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    legitimate_amounts = df[df['is_fraud'] == 0]['amt']\n",
    "    fraud_amounts = df[df['is_fraud'] == 1]['amt']\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    statistic, p_value = stats.mannwhitneyu(legitimate_amounts, fraud_amounts, alternative='two-sided')\n",
    "    \n",
    "    print(\"\\nüßÆ Statistical Tests:\")\n",
    "    print(f\"   Mann-Whitney U Test p-value: {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úÖ Significant difference in amounts between fraud and legitimate transactions\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No significant difference in amounts\")\n",
    "    \n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(f\"   - Average legitimate amount: ${legitimate_amounts.mean():.2f}\")\n",
    "    print(f\"   - Average fraud amount: ${fraud_amounts.mean():.2f}\")\n",
    "    print(f\"   - Median legitimate amount: ${legitimate_amounts.median():.2f}\")\n",
    "    print(f\"   - Median fraud amount: ${fraud_amounts.median():.2f}\")\n",
    "\n",
    "analyze_transaction_amounts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Temporal Pattern Analysis\n",
    "def analyze_temporal_patterns(df):\n",
    "    \"\"\"Analyze fraud patterns over time\"\"\"\n",
    "    \n",
    "    print(\"‚è∞ Temporal Pattern Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    \n",
    "    # Create temporal visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Fraud Rate by Hour of Day',\n",
    "            'Fraud Rate by Day of Week',\n",
    "            'Transaction Volume by Hour',\n",
    "            'Monthly Fraud Trends'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Fraud rate by hour\n",
    "    hourly_fraud = df.groupby('hour')['is_fraud'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=hourly_fraud['hour'], y=hourly_fraud['mean'], \n",
    "                  mode='lines+markers', name='Fraud Rate by Hour'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Fraud rate by day of week\n",
    "    daily_fraud = df.groupby('day_of_week')['is_fraud'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    # Reorder days\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    daily_fraud['day_of_week'] = pd.Categorical(daily_fraud['day_of_week'], categories=day_order, ordered=True)\n",
    "    daily_fraud = daily_fraud.sort_values('day_of_week')\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=daily_fraud['day_of_week'], y=daily_fraud['mean'], name='Fraud Rate by Day'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Transaction volume by hour\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=hourly_fraud['hour'], y=hourly_fraud['count'], name='Transaction Count'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Monthly trends\n",
    "    monthly_fraud = df.groupby('month')['is_fraud'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=monthly_fraud['month'], y=monthly_fraud['mean'], \n",
    "                  mode='lines+markers', name='Monthly Fraud Rate'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Temporal Pattern Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Time-based insights\n",
    "    print(\"‚è∞ Temporal Insights:\")\n",
    "    \n",
    "    # Peak fraud hours\n",
    "    peak_fraud_hour = hourly_fraud.loc[hourly_fraud['mean'].idxmax(), 'hour']\n",
    "    peak_fraud_rate = hourly_fraud['mean'].max()\n",
    "    print(f\"   - Peak fraud hour: {peak_fraud_hour}:00 ({peak_fraud_rate:.2%} fraud rate)\")\n",
    "    \n",
    "    # Safest hours\n",
    "    safest_hour = hourly_fraud.loc[hourly_fraud['mean'].idxmin(), 'hour']\n",
    "    safest_rate = hourly_fraud['mean'].min()\n",
    "    print(f\"   - Safest hour: {safest_hour}:00 ({safest_rate:.2%} fraud rate)\")\n",
    "    \n",
    "    # Day of week patterns\n",
    "    highest_fraud_day = daily_fraud.loc[daily_fraud['mean'].idxmax(), 'day_of_week']\n",
    "    lowest_fraud_day = daily_fraud.loc[daily_fraud['mean'].idxmin(), 'day_of_week']\n",
    "    print(f\"   - Highest fraud day: {highest_fraud_day}\")\n",
    "    print(f\"   - Lowest fraud day: {lowest_fraud_day}\")\n",
    "    \n",
    "    # Business hours analysis\n",
    "    business_hours = df[df['hour'].between(9, 17)]\n",
    "    after_hours = df[~df['hour'].between(9, 17)]\n",
    "    \n",
    "    print(f\"   - Business hours fraud rate: {business_hours['is_fraud'].mean():.2%}\")\n",
    "    print(f\"   - After hours fraud rate: {after_hours['is_fraud'].mean():.2%}\")\n",
    "\n",
    "analyze_temporal_patterns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ecbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Geographic Analysis\n",
    "def analyze_geographic_patterns(df):\n",
    "    \"\"\"Analyze geographic fraud patterns\"\"\"\n",
    "    \n",
    "    print(\"üó∫Ô∏è Geographic Pattern Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate distance between customer and merchant\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance using Haversine formula\"\"\"\n",
    "        from math import radians, cos, sin, asin, sqrt\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        return c * 6371  # Earth radius in km\n",
    "    \n",
    "    df['distance_km'] = df.apply(\n",
    "        lambda x: haversine_distance(x['lat'], x['long'], x['merch_lat'], x['merch_long']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create geographic visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Customer Locations (Fraud vs Legitimate)',\n",
    "            'Distance Distribution',\n",
    "            'Fraud Rate by State',\n",
    "            'Distance vs Fraud Rate'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"scattergeo\"}, {\"type\": \"histogram\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Geographic scatter plot\n",
    "    fraud_transactions = df[df['is_fraud'] == 1]\n",
    "    legitimate_transactions = df[df['is_fraud'] == 0].sample(n=min(1000, len(df[df['is_fraud'] == 0])))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            lon=legitimate_transactions['long'],\n",
    "            lat=legitimate_transactions['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, color='blue', opacity=0.6),\n",
    "            name='Legitimate'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scattergeo(\n",
    "            lon=fraud_transactions['long'],\n",
    "            lat=fraud_transactions['lat'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=6, color='red', opacity=0.8),\n",
    "            name='Fraud'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Distance distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[df['is_fraud'] == 0]['distance_km'], name='Legitimate', opacity=0.7, nbinsx=50),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[df['is_fraud'] == 1]['distance_km'], name='Fraud', opacity=0.7, nbinsx=50),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Fraud rate by state\n",
    "    state_fraud = df.groupby('state')['is_fraud'].agg(['count', 'sum', 'mean']).reset_index()\n",
    "    state_fraud = state_fraud[state_fraud['count'] >= 10]  # Filter states with enough data\n",
    "    state_fraud = state_fraud.sort_values('mean', ascending=False)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=state_fraud['state'], y=state_fraud['mean'], name='Fraud Rate by State'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Distance vs fraud rate\n",
    "    df['distance_bin'] = pd.cut(df['distance_km'], bins=20, labels=False)\n",
    "    distance_fraud = df.groupby('distance_bin')['is_fraud'].agg(['count', 'mean']).reset_index()\n",
    "    distance_fraud = distance_fraud[distance_fraud['count'] >= 10]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=distance_fraud['distance_bin'], y=distance_fraud['mean'], \n",
    "                  mode='lines+markers', name='Fraud Rate vs Distance'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_geos(scope=\"usa\")\n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Geographic Pattern Analysis\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Geographic insights\n",
    "    print(\"üó∫Ô∏è Geographic Insights:\")\n",
    "    print(f\"   - Average distance (legitimate): {df[df['is_fraud'] == 0]['distance_km'].mean():.2f} km\")\n",
    "    print(f\"   - Average distance (fraud): {df[df['is_fraud'] == 1]['distance_km'].mean():.2f} km\")\n",
    "    print(f\"   - Max distance: {df['distance_km'].max():.2f} km\")\n",
    "    \n",
    "    # Distance analysis\n",
    "    short_distance = df[df['distance_km'] < 10]\n",
    "    long_distance = df[df['distance_km'] > 100]\n",
    "    \n",
    "    print(f\"   - Short distance (<10km) fraud rate: {short_distance['is_fraud'].mean():.2%}\")\n",
    "    print(f\"   - Long distance (>100km) fraud rate: {long_distance['is_fraud'].mean():.2%}\")\n",
    "    \n",
    "    # Top fraud states\n",
    "    if len(state_fraud) > 0:\n",
    "        top_fraud_state = state_fraud.iloc[0]\n",
    "        print(f\"   - Highest fraud state: {top_fraud_state['state']} ({top_fraud_state['mean']:.2%})\")\n",
    "\n",
    "analyze_geographic_patterns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e92e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Category and Merchant Analysis\n",
    "def analyze_categories_merchants(df):\n",
    "    \"\"\"Analyze fraud patterns by transaction category and merchant\"\"\"\n",
    "    \n",
    "    print(\"üè™ Category and Merchant Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Category analysis\n",
    "    category_analysis = df.groupby('category').agg({\n",
    "        'is_fraud': ['count', 'sum', 'mean'],\n",
    "        'amt': ['mean', 'median']\n",
    "    }).round(4)\n",
    "    \n",
    "    category_analysis.columns = ['transaction_count', 'fraud_count', 'fraud_rate', 'avg_amount', 'median_amount']\n",
    "    category_analysis = category_analysis.sort_values('fraud_rate', ascending=False)\n",
    "    \n",
    "    print(\"üìä Category Analysis:\")\n",
    "    print(category_analysis)\n",
    "    \n",
    "    # Merchant analysis (top merchants by transaction volume)\n",
    "    merchant_analysis = df.groupby('merchant').agg({\n",
    "        'is_fraud': ['count', 'sum', 'mean'],\n",
    "        'amt': ['mean']\n",
    "    }).round(4)\n",
    "    \n",
    "    merchant_analysis.columns = ['transaction_count', 'fraud_count', 'fraud_rate', 'avg_amount']\n",
    "    merchant_analysis = merchant_analysis[merchant_analysis['transaction_count'] >= 5]  # Filter for reliability\n",
    "    merchant_analysis = merchant_analysis.sort_values('fraud_rate', ascending=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Fraud Rate by Category',\n",
    "            'Transaction Volume by Category',\n",
    "            'Top 20 Riskiest Merchants',\n",
    "            'Amount Distribution by Category'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Fraud rate by category\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=category_analysis.index, y=category_analysis['fraud_rate'], \n",
    "               name='Fraud Rate by Category'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Transaction volume by category\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=category_analysis.index, y=category_analysis['transaction_count'], \n",
    "               name='Transaction Count'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Top risky merchants\n",
    "    top_risky_merchants = merchant_analysis.head(20)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_risky_merchants.index, y=top_risky_merchants['fraud_rate'], \n",
    "               name='Top Risky Merchants'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Amount distribution by category (box plot)\n",
    "    for i, category in enumerate(df['category'].unique()):\n",
    "        category_data = df[df['category'] == category]['amt']\n",
    "        fig.add_trace(\n",
    "            go.Box(y=category_data, name=category, showlegend=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Category and Merchant Analysis\")\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.show()\n",
    "    \n",
    "    # Statistical analysis\n",
    "    print(\"\\nüìà Category Insights:\")\n",
    "    \n",
    "    # Highest and lowest risk categories\n",
    "    highest_risk_cat = category_analysis.index[0]\n",
    "    lowest_risk_cat = category_analysis.index[-1]\n",
    "    \n",
    "    print(f\"   - Highest risk category: {highest_risk_cat} ({category_analysis.loc[highest_risk_cat, 'fraud_rate']:.2%})\")\n",
    "    print(f\"   - Lowest risk category: {lowest_risk_cat} ({category_analysis.loc[lowest_risk_cat, 'fraud_rate']:.2%})\")\n",
    "    \n",
    "    # Chi-square test for category independence\n",
    "    contingency_table = pd.crosstab(df['category'], df['is_fraud'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(\"\\nüßÆ Statistical Tests:\")\n",
    "    print(f\"   - Chi-square test p-value: {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úÖ Significant association between category and fraud\")\n",
    "    else:\n",
    "        print(\"   ‚ùå No significant association between category and fraud\")\n",
    "    \n",
    "    # Merchant insights\n",
    "    print(\"\\nüè™ Merchant Insights:\")\n",
    "    print(f\"   - Total unique merchants: {df['merchant'].nunique()}\")\n",
    "    print(f\"   - Merchants with fraud: {merchant_analysis[merchant_analysis['fraud_count'] > 0].shape[0]}\")\n",
    "    \n",
    "    if len(merchant_analysis) > 0:\n",
    "        avg_merchant_fraud_rate = merchant_analysis['fraud_rate'].mean()\n",
    "        print(f\"   - Average merchant fraud rate: {avg_merchant_fraud_rate:.2%}\")\n",
    "\n",
    "analyze_categories_merchants(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1308816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Customer Demographics Analysis\n",
    "def analyze_demographics(df):\n",
    "    \"\"\"Analyze fraud patterns by customer demographics\"\"\"\n",
    "    \n",
    "    print(\"üë• Customer Demographics Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate age from date of birth\n",
    "    df['dob_date'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "    current_date = pd.Timestamp.now()\n",
    "    df['age'] = (current_date - df['dob_date']).dt.days / 365.25\n",
    "    df['age'] = df['age'].fillna(df['age'].median())  # Fill missing ages\n",
    "    \n",
    "    # Create age bins\n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                            bins=[0, 25, 35, 50, 65, 100], \n",
    "                            labels=['18-25', '26-35', '36-50', '51-65', '65+'])\n",
    "    \n",
    "    # Gender analysis\n",
    "    gender_analysis = df.groupby('gender')['is_fraud'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    gender_analysis.columns = ['transaction_count', 'fraud_count', 'fraud_rate']\n",
    "    \n",
    "    # Age group analysis\n",
    "    age_analysis = df.groupby('age_group')['is_fraud'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    age_analysis.columns = ['transaction_count', 'fraud_count', 'fraud_rate']\n",
    "    \n",
    "    # Job analysis\n",
    "    job_analysis = df.groupby('job')['is_fraud'].agg(['count', 'sum', 'mean']).round(4)\n",
    "    job_analysis.columns = ['transaction_count', 'fraud_count', 'fraud_rate']\n",
    "    job_analysis = job_analysis.sort_values('fraud_rate', ascending=False)\n",
    "    \n",
    "    print(\"üë§ Demographic Analysis Results:\")\n",
    "    print(\"\\nüìä Gender Analysis:\")\n",
    "    print(gender_analysis)\n",
    "    \n",
    "    print(\"\\nüìä Age Group Analysis:\")\n",
    "    print(age_analysis)\n",
    "    \n",
    "    print(\"\\nüìä Top 10 Jobs by Fraud Rate:\")\n",
    "    print(job_analysis.head(10))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Fraud Rate by Gender',\n",
    "            'Fraud Rate by Age Group',\n",
    "            'Age Distribution (Fraud vs Legitimate)',\n",
    "            'Top 10 Jobs by Fraud Rate'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Gender analysis\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=gender_analysis.index, y=gender_analysis['fraud_rate'], name='Gender'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Age group analysis\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=age_analysis.index, y=age_analysis['fraud_rate'], name='Age Group'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Age distribution\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[df['is_fraud'] == 0]['age'], name='Legitimate', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[df['is_fraud'] == 1]['age'], name='Fraud', opacity=0.7, nbinsx=30),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Top jobs by fraud rate\n",
    "    top_jobs = job_analysis.head(10)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_jobs.index, y=top_jobs['fraud_rate'], name='Job Risk'),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, showlegend=True, title_text=\"Customer Demographics Analysis\")\n",
    "    fig.update_xaxes(tickangle=45, row=2, col=2)\n",
    "    fig.show()\n",
    "    \n",
    "    # Demographic insights\n",
    "    print(\"\\nüí° Demographic Insights:\")\n",
    "    \n",
    "    # Age insights\n",
    "    fraud_ages = df[df['is_fraud'] == 1]['age']\n",
    "    legitimate_ages = df[df['is_fraud'] == 0]['age']\n",
    "    \n",
    "    print(f\"   - Average fraud customer age: {fraud_ages.mean():.1f} years\")\n",
    "    print(f\"   - Average legitimate customer age: {legitimate_ages.mean():.1f} years\")\n",
    "    \n",
    "    # Gender insights\n",
    "    if len(gender_analysis) > 1:\n",
    "        high_risk_gender = gender_analysis['fraud_rate'].idxmax()\n",
    "        print(f\"   - Higher risk gender: {high_risk_gender} ({gender_analysis.loc[high_risk_gender, 'fraud_rate']:.2%})\")\n",
    "    \n",
    "    # Age group insights\n",
    "    high_risk_age = age_analysis['fraud_rate'].idxmax()\n",
    "    low_risk_age = age_analysis['fraud_rate'].idxmin()\n",
    "    print(f\"   - Highest risk age group: {high_risk_age} ({age_analysis.loc[high_risk_age, 'fraud_rate']:.2%})\")\n",
    "    print(f\"   - Lowest risk age group: {low_risk_age} ({age_analysis.loc[low_risk_age, 'fraud_rate']:.2%})\")\n",
    "\n",
    "analyze_demographics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebff17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Correlation Analysis\n",
    "def analyze_correlations(df):\n",
    "    \"\"\"Analyze correlations between numerical features and fraud\"\"\"\n",
    "    \n",
    "    print(\"üîó Correlation Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Select numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove non-meaningful correlations\n",
    "    exclude_cols = ['cc_num', 'merch_zipcode', 'unix_time'] if any(col in numerical_cols for col in ['cc_num', 'merch_zipcode', 'unix_time']) else []\n",
    "    numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.columns,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,\n",
    "        text=correlation_matrix.round(3).values,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Feature Correlation Matrix\",\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Fraud correlations\n",
    "    fraud_correlations = correlation_matrix['is_fraud'].abs().sort_values(ascending=False)\n",
    "    fraud_correlations = fraud_correlations[fraud_correlations.index != 'is_fraud']\n",
    "    \n",
    "    print(\"üéØ Features Most Correlated with Fraud:\")\n",
    "    for feature, correlation in fraud_correlations.head(10).items():\n",
    "        print(f\"   {feature}: {correlation:.4f}\")\n",
    "    \n",
    "    # Feature relationships\n",
    "    print(\"\\nüîç Key Relationships:\")\n",
    "    \n",
    "    # Strong positive correlations (excluding fraud)\n",
    "    upper_triangle = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    strong_correlations = []\n",
    "    for col in upper_triangle.columns:\n",
    "        for idx in upper_triangle.index:\n",
    "            if abs(upper_triangle.loc[idx, col]) > 0.7:\n",
    "                strong_correlations.append((idx, col, upper_triangle.loc[idx, col]))\n",
    "    \n",
    "    if strong_correlations:\n",
    "        print(\"   Strong correlations (>0.7):\")\n",
    "        for feat1, feat2, corr in strong_correlations[:5]:\n",
    "            print(f\"     {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "correlation_matrix = analyze_correlations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21eadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Summary and Key Findings\n",
    "def generate_summary(df):\n",
    "    \"\"\"Generate comprehensive summary of findings\"\"\"\n",
    "    \n",
    "    print(\"üìã FRAUD DETECTION - DATA ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset overview\n",
    "    print(\"üìä Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total transactions: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Fraud transactions: {df['is_fraud'].sum():,}\")\n",
    "    print(f\"   ‚Ä¢ Fraud rate: {df['is_fraud'].mean():.2%}\")\n",
    "    print(f\"   ‚Ä¢ Time period: {df['trans_date_trans_time'].min()} to {df['trans_date_trans_time'].max()}\")\n",
    "    \n",
    "    # Key fraud patterns\n",
    "    print(\"\\nüéØ Key Fraud Patterns Identified:\")\n",
    "    \n",
    "    # Amount patterns\n",
    "    fraud_amounts = df[df['is_fraud'] == 1]['amt']\n",
    "    legitimate_amounts = df[df['is_fraud'] == 0]['amt']\n",
    "    \n",
    "    if fraud_amounts.mean() > legitimate_amounts.mean():\n",
    "        print(\"   ‚Ä¢ Fraud transactions have higher average amounts\")\n",
    "        print(f\"     - Fraud avg: ${fraud_amounts.mean():.2f}\")\n",
    "        print(f\"     - Legitimate avg: ${legitimate_amounts.mean():.2f}\")\n",
    "    \n",
    "    # Temporal patterns\n",
    "    df['hour'] = pd.to_datetime(df['trans_date_trans_time']).dt.hour\n",
    "    hourly_fraud = df.groupby('hour')['is_fraud'].mean()\n",
    "    peak_hour = hourly_fraud.idxmax()\n",
    "    print(f\"   ‚Ä¢ Peak fraud hour: {peak_hour}:00 ({hourly_fraud[peak_hour]:.2%} fraud rate)\")\n",
    "    \n",
    "    # Geographic patterns\n",
    "    if 'distance_km' in df.columns:\n",
    "        long_distance_fraud = df[df['distance_km'] > 100]['is_fraud'].mean()\n",
    "        short_distance_fraud = df[df['distance_km'] <= 10]['is_fraud'].mean()\n",
    "        print(f\"   ‚Ä¢ Long distance transactions (>100km) have {long_distance_fraud:.2%} fraud rate\")\n",
    "        print(f\"   ‚Ä¢ Short distance transactions (‚â§10km) have {short_distance_fraud:.2%} fraud rate\")\n",
    "    \n",
    "    # Category patterns\n",
    "    category_fraud = df.groupby('category')['is_fraud'].mean().sort_values(ascending=False)\n",
    "    highest_risk_category = category_fraud.index[0]\n",
    "    print(f\"   ‚Ä¢ Highest risk category: {highest_risk_category} ({category_fraud.iloc[0]:.2%})\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    print(\"\\n‚úÖ Data Quality:\")\n",
    "    missing_percentage = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
    "    print(f\"   ‚Ä¢ Missing data: {missing_percentage:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Duplicate transactions: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Feature recommendations\n",
    "    print(\"\\nüîß Recommended Features for Modeling:\")\n",
    "    \n",
    "    # Based on correlation analysis\n",
    "    if 'distance_km' in df.columns:\n",
    "        print(\"   ‚Ä¢ Transaction amount (continuous and log-transformed)\")\n",
    "        print(\"   ‚Ä¢ Customer-merchant distance\")\n",
    "        print(\"   ‚Ä¢ Time-based features (hour, day of week)\")\n",
    "        print(\"   ‚Ä¢ Geographic features (state, city population)\")\n",
    "        print(\"   ‚Ä¢ Category encoding\")\n",
    "        print(\"   ‚Ä¢ Customer demographics (age, gender)\")\n",
    "    \n",
    "    # Model considerations\n",
    "    print(\"\\nü§ñ Modeling Considerations:\")\n",
    "    print(\"   ‚Ä¢ Class imbalance: Consider SMOTE, class weights, or threshold tuning\")\n",
    "    print(\"   ‚Ä¢ Feature scaling: Standardize numerical features\")\n",
    "    print(\"   ‚Ä¢ Categorical encoding: Use target encoding for high-cardinality features\")\n",
    "    print(\"   ‚Ä¢ Cross-validation: Use time-based splits for temporal data\")\n",
    "    print(\"   ‚Ä¢ Evaluation metrics: Focus on precision, recall, and AUC\")\n",
    "    \n",
    "    # Business insights\n",
    "    print(\"\\nüíº Business Insights:\")\n",
    "    total_fraud_amount = df[df['is_fraud'] == 1]['amt'].sum()\n",
    "    print(f\"   ‚Ä¢ Total fraud amount in dataset: ${total_fraud_amount:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Average fraud loss per transaction: ${fraud_amounts.mean():.2f}\")\n",
    "    \n",
    "    if len(category_fraud) > 1:\n",
    "        print(f\"   ‚Ä¢ Focus fraud prevention on {highest_risk_category} category\")\n",
    "    \n",
    "    print(\"   ‚Ä¢ Implement real-time distance checking for transactions >100km\")\n",
    "    print(f\"   ‚Ä¢ Enhanced monitoring during peak fraud hours ({peak_hour}:00)\")\n",
    "    \n",
    "    print(\"\\nüéâ Analysis Complete! Dataset is ready for feature engineering and modeling.\")\n",
    "\n",
    "generate_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdae912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Export Processed Data\n",
    "def export_processed_data(df):\n",
    "    \"\"\"Export processed data for modeling\"\"\"\n",
    "    \n",
    "    print(\"üíæ Exporting Processed Data\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs('../data/processed/', exist_ok=True)\n",
    "    \n",
    "    # Add engineered features used in analysis\n",
    "    if 'distance_km' not in df.columns:\n",
    "        # Add distance calculation if not already present\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            from math import radians, cos, sin, asin, sqrt\n",
    "            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "            c = 2 * asin(sqrt(a))\n",
    "            return c * 6371\n",
    "        \n",
    "        df['distance_km'] = df.apply(\n",
    "            lambda x: haversine_distance(x['lat'], x['long'], x['merch_lat'], x['merch_long']), \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Add time features\n",
    "    df['datetime'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Export to multiple formats\n",
    "    \n",
    "    # 1. Full dataset as parquet (efficient storage)\n",
    "    df.to_parquet('../data/processed/fraud_data_analyzed.parquet', index=False)\n",
    "    print(\"‚úÖ Exported full dataset to: fraud_data_analyzed.parquet\")\n",
    "    \n",
    "    # 2. CSV for compatibility\n",
    "    df.to_csv('../data/processed/fraud_data_analyzed.csv', index=False)\n",
    "    print(\"‚úÖ Exported full dataset to: fraud_data_analyzed.csv\")\n",
    "    \n",
    "    # 3. Split into train/validation/test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # First split: train+val vs test (80-20)\n",
    "    train_val, test = train_test_split(df, test_size=0.2, stratify=df['is_fraud'], random_state=42)\n",
    "    \n",
    "    # Second split: train vs val (80-20 of remaining)\n",
    "    train, val = train_test_split(train_val, test_size=0.25, stratify=train_val['is_fraud'], random_state=42)\n",
    "    \n",
    "    # Export splits\n",
    "    train.to_parquet('../data/processed/train_analyzed.parquet', index=False)\n",
    "    val.to_parquet('../data/processed/validation_analyzed.parquet', index=False)\n",
    "    test.to_parquet('../data/processed/test_analyzed.parquet', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Exported train set: {len(train):,} samples ({train['is_fraud'].mean():.2%} fraud)\")\n",
    "    print(f\"‚úÖ Exported validation set: {len(val):,} samples ({val['is_fraud'].mean():.2%} fraud)\")\n",
    "    print(f\"‚úÖ Exported test set: {len(test):,} samples ({test['is_fraud'].mean():.2%} fraud)\")\n",
    "    \n",
    "    # 4. Export feature summary\n",
    "    feature_summary = {\n",
    "        'total_features': len(df.columns),\n",
    "        'numerical_features': list(df.select_dtypes(include=[np.number]).columns),\n",
    "        'categorical_features': list(df.select_dtypes(include=['object', 'category']).columns),\n",
    "        'target_variable': 'is_fraud',\n",
    "        'fraud_rate': float(df['is_fraud'].mean()),\n",
    "        'sample_size': len(df),\n",
    "        'date_range': {\n",
    "            'start': str(df['trans_date_trans_time'].min()),\n",
    "            'end': str(df['trans_date_trans_time'].max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('../data/processed/feature_summary.json', 'w') as f:\n",
    "        json.dump(feature_summary, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Exported feature summary to: feature_summary.json\")\n",
    "    print(f\"\\nüéØ Ready for model training with {len(df)} samples and {len(df.columns)} features!\")\n",
    "\n",
    "export_processed_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üéâ EXPLORATORY DATA ANALYSIS COMPLETE!\n",
    "=====================================\n",
    "\n",
    "üìÅ Files Created:\n",
    "   ‚Ä¢ fraud_data_analyzed.parquet/csv - Full processed dataset\n",
    "   ‚Ä¢ train_analyzed.parquet - Training set\n",
    "   ‚Ä¢ validation_analyzed.parquet - Validation set  \n",
    "   ‚Ä¢ test_analyzed.parquet - Test set\n",
    "   ‚Ä¢ feature_summary.json - Feature metadata\n",
    "\n",
    "üöÄ Next Steps:\n",
    "   1. Run feature engineering notebook (02_feature_engineering.ipynb)\n",
    "   2. Train models using processed data (03_model_training.ipynb)\n",
    "   3. Evaluate model performance (04_model_evaluation.ipynb)\n",
    "   4. Deploy best model to production API\n",
    "\n",
    "üí° Key Findings:\n",
    "   ‚Ä¢ Fraud patterns identified in amounts, time, and geography\n",
    "   ‚Ä¢ Data quality is suitable for machine learning\n",
    "   ‚Ä¢ Clear feature candidates for model training\n",
    "   ‚Ä¢ Business insights for fraud prevention strategy\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diskominfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
