{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29858823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/03_model_training.ipynb\n",
    "\n",
    "\"\"\"\n",
    "Fraud Detection Model Training Notebook\n",
    "========================================\n",
    "\n",
    "This notebook demonstrates the complete model training pipeline\n",
    "for the fraud detection system.\n",
    "\"\"\"\n",
    "\n",
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Cell 2: Load and Explore Data\n",
    "def load_data():\n",
    "    \"\"\"Load the fraud detection dataset\"\"\"\n",
    "    try:\n",
    "        # Try to load processed data first\n",
    "        train_data = pd.read_parquet('../data/processed/train.parquet')\n",
    "        val_data = pd.read_parquet('../data/processed/validation.parquet')\n",
    "        test_data = pd.read_parquet('../data/processed/test.parquet')\n",
    "        print(\"âœ… Loaded processed data\")\n",
    "    except:\n",
    "        # If not available, load raw data\n",
    "        train_data = pd.read_csv('../data/raw/train_data.csv')\n",
    "        val_data = pd.read_csv('../data/raw/val_data.csv')\n",
    "        test_data = pd.read_csv('../data/raw/test_data.csv')\n",
    "        print(\"âœ… Loaded raw data\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df = load_data()\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Basic info\n",
    "print(\"\\nğŸ“Š Dataset Info:\")\n",
    "print(f\"Total samples: {len(train_df) + len(val_df) + len(test_df):,}\")\n",
    "print(f\"Fraud rate in training: {train_df['is_fraud'].mean():.3%}\")\n",
    "print(f\"Features: {train_df.shape[1]}\")\n",
    "\n",
    "# Cell 3: Exploratory Data Analysis\n",
    "def plot_fraud_distribution(df):\n",
    "    \"\"\"Plot fraud vs legitimate transactions\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    fraud_counts = df['is_fraud'].value_counts()\n",
    "    axes[0].pie(fraud_counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.1f%%',\n",
    "               colors=['#2E8B57', '#FF6B6B'])\n",
    "    axes[0].set_title('Transaction Distribution')\n",
    "    \n",
    "    # Amount distribution by fraud\n",
    "    sns.boxplot(data=df, x='is_fraud', y='amt', ax=axes[1])\n",
    "    axes[1].set_title('Transaction Amount by Fraud Status')\n",
    "    axes[1].set_xlabel('Is Fraud')\n",
    "    axes[1].set_ylabel('Amount ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_correlations(df):\n",
    "    \"\"\"Plot correlation matrix of numerical features\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_category_analysis(df):\n",
    "    \"\"\"Analyze fraud by transaction category\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Fraud rate by category\n",
    "    fraud_by_category = df.groupby('category')['is_fraud'].agg(['count', 'sum', 'mean'])\n",
    "    fraud_by_category['fraud_rate'] = fraud_by_category['mean']\n",
    "    \n",
    "    fraud_by_category['fraud_rate'].plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Fraud Rate by Transaction Category')\n",
    "    axes[0].set_ylabel('Fraud Rate')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Transaction volume by category\n",
    "    fraud_by_category['count'].plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title('Transaction Volume by Category')\n",
    "    axes[1].set_ylabel('Number of Transactions')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fraud_by_category\n",
    "\n",
    "# Run EDA\n",
    "print(\"ğŸ” Exploratory Data Analysis\")\n",
    "plot_fraud_distribution(train_df)\n",
    "plot_feature_correlations(train_df)\n",
    "category_analysis = plot_category_analysis(train_df)\n",
    "\n",
    "print(\"\\nFraud Rate by Category:\")\n",
    "print(category_analysis.round(3))\n",
    "\n",
    "# Cell 4: Feature Engineering\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"Create advanced features for fraud detection\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    print(\"ğŸ”§ Creating advanced features...\")\n",
    "    \n",
    "    # 1. Basic mathematical transformations\n",
    "    features['amt_log'] = np.log1p(features['amt'])\n",
    "    features['amt_sqrt'] = np.sqrt(features['amt'])\n",
    "    features['city_pop_log'] = np.log1p(features['city_pop'])\n",
    "    \n",
    "    # 2. Geographic features\n",
    "    def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"Calculate distance between two points using Haversine formula\"\"\"\n",
    "        from math import radians, cos, sin, asin, sqrt\n",
    "        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        return c * 6371  # Earth radius in km\n",
    "    \n",
    "    features['customer_merchant_distance'] = features.apply(\n",
    "        lambda x: haversine_distance(x['lat'], x['long'], x['merch_lat'], x['merch_long']), axis=1\n",
    "    )\n",
    "    \n",
    "    # Distance categories\n",
    "    features['distance_category'] = pd.cut(\n",
    "        features['customer_merchant_distance'],\n",
    "        bins=[0, 10, 50, 200, float('inf')],\n",
    "        labels=['very_close', 'close', 'far', 'very_far']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # 3. Temporal features\n",
    "    features['trans_datetime'] = pd.to_datetime(features['trans_date_trans_time'])\n",
    "    features['hour'] = features['trans_datetime'].dt.hour\n",
    "    features['day_of_week'] = features['trans_datetime'].dt.dayofweek\n",
    "    features['day_of_month'] = features['trans_datetime'].dt.day\n",
    "    features['month'] = features['trans_datetime'].dt.month\n",
    "    \n",
    "    # Time-based binary features\n",
    "    features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)\n",
    "    features['is_night'] = ((features['hour'] < 6) | (features['hour'] > 22)).astype(int)\n",
    "    features['is_business_hours'] = ((features['hour'] >= 9) & (features['hour'] <= 17)).astype(int)\n",
    "    features['is_late_night'] = ((features['hour'] >= 23) | (features['hour'] <= 5)).astype(int)\n",
    "    \n",
    "    # 4. Customer demographic features\n",
    "    features['dob'] = pd.to_datetime(features['dob'], errors='coerce')\n",
    "    features['customer_age'] = (datetime.now() - features['dob']).dt.days / 365.25\n",
    "    features['customer_age'] = features['customer_age'].fillna(features['customer_age'].median())\n",
    "    \n",
    "    # Age categories\n",
    "    features['age_category'] = pd.cut(\n",
    "        features['customer_age'],\n",
    "        bins=[0, 25, 40, 60, float('inf')],\n",
    "        labels=['young', 'adult', 'middle_aged', 'senior']\n",
    "    ).astype(str)\n",
    "    \n",
    "    # 5. Amount-based features\n",
    "    features['amt_zscore'] = (features['amt'] - features['amt'].mean()) / features['amt'].std()\n",
    "    features['is_high_amount'] = (features['amt'] > features['amt'].quantile(0.9)).astype(int)\n",
    "    features['is_round_amount'] = (features['amt'] % 1 == 0).astype(int)\n",
    "    \n",
    "    # 6. Categorical encoding\n",
    "    le_gender = LabelEncoder()\n",
    "    features['gender_encoded'] = le_gender.fit_transform(features['gender'])\n",
    "    \n",
    "    le_category = LabelEncoder()\n",
    "    features['category_encoded'] = le_category.fit_transform(features['category'])\n",
    "    \n",
    "    le_state = LabelEncoder()\n",
    "    features['state_encoded'] = le_state.fit_transform(features['state'])\n",
    "    \n",
    "    le_distance_cat = LabelEncoder()\n",
    "    features['distance_category_encoded'] = le_distance_cat.fit_transform(features['distance_category'])\n",
    "    \n",
    "    le_age_cat = LabelEncoder()\n",
    "    features['age_category_encoded'] = le_age_cat.fit_transform(features['age_category'])\n",
    "    \n",
    "    # 7. Risk scoring features (based on domain knowledge)\n",
    "    # High risk if transaction is at unusual time + high amount + far distance\n",
    "    features['risk_score'] = (\n",
    "        features['is_night'] * 0.3 +\n",
    "        features['is_high_amount'] * 0.4 +\n",
    "        (features['customer_merchant_distance'] > 100).astype(int) * 0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Created {len(features.columns) - len(df.columns)} new features\")\n",
    "    return features\n",
    "\n",
    "# Apply feature engineering\n",
    "train_features = create_advanced_features(train_df)\n",
    "val_features = create_advanced_features(val_df)\n",
    "test_features = create_advanced_features(test_df)\n",
    "\n",
    "print(f\"Original features: {train_df.shape[1]}\")\n",
    "print(f\"Enhanced features: {train_features.shape[1]}\")\n",
    "\n",
    "# Cell 5: Feature Selection\n",
    "def select_features(train_df, val_df, target_col='is_fraud'):\n",
    "    \"\"\"Select the best features for modeling\"\"\"\n",
    "    \n",
    "    # Define feature categories\n",
    "    basic_features = [\n",
    "        'amt', 'amt_log', 'amt_sqrt', 'amt_zscore',\n",
    "        'city_pop', 'city_pop_log',\n",
    "        'customer_merchant_distance'\n",
    "    ]\n",
    "    \n",
    "    temporal_features = [\n",
    "        'hour', 'day_of_week', 'is_weekend', 'is_night', \n",
    "        'is_business_hours', 'is_late_night'\n",
    "    ]\n",
    "    \n",
    "    demographic_features = [\n",
    "        'customer_age', 'gender_encoded', 'age_category_encoded'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'category_encoded', 'state_encoded', 'distance_category_encoded'\n",
    "    ]\n",
    "    \n",
    "    risk_features = [\n",
    "        'is_high_amount', 'is_round_amount', 'risk_score'\n",
    "    ]\n",
    "    \n",
    "    # Combine all feature groups\n",
    "    selected_features = (basic_features + temporal_features + \n",
    "                        demographic_features + categorical_features + risk_features)\n",
    "    \n",
    "    # Ensure all features exist in dataframe\n",
    "    available_features = [f for f in selected_features if f in train_df.columns]\n",
    "    \n",
    "    print(f\"Selected {len(available_features)} features:\")\n",
    "    for category, features in [\n",
    "        (\"Basic\", basic_features),\n",
    "        (\"Temporal\", temporal_features), \n",
    "        (\"Demographic\", demographic_features),\n",
    "        (\"Categorical\", categorical_features),\n",
    "        (\"Risk\", risk_features)\n",
    "    ]:\n",
    "        valid_features = [f for f in features if f in available_features]\n",
    "        print(f\"  {category}: {valid_features}\")\n",
    "    \n",
    "    return available_features\n",
    "\n",
    "# Select features\n",
    "feature_columns = select_features(train_features, val_features)\n",
    "\n",
    "# Prepare feature matrices\n",
    "X_train = train_features[feature_columns]\n",
    "y_train = train_features['is_fraud']\n",
    "X_val = val_features[feature_columns]\n",
    "y_val = val_features['is_fraud']\n",
    "X_test = test_features[feature_columns]\n",
    "y_test = test_features['is_fraud']\n",
    "\n",
    "print(\"\\nFeature matrix shapes:\")\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Cell 6: Model Training and Comparison\n",
    "def train_multiple_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and compare multiple models\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    results = {}\n",
    "    \n",
    "    print(\"ğŸ¤– Training multiple models...\")\n",
    "    \n",
    "    # 1. Logistic Regression (baseline)\n",
    "    print(\"\\n1. Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_pred = lr_model.predict(X_val)\n",
    "    lr_prob = lr_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    models['Logistic Regression'] = lr_model\n",
    "    results['Logistic Regression'] = {\n",
    "        'accuracy': (lr_pred == y_val).mean(),\n",
    "        'auc': roc_auc_score(y_val, lr_prob),\n",
    "        'predictions': lr_pred,\n",
    "        'probabilities': lr_prob\n",
    "    }\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"2. Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_pred = rf_model.predict(X_val)\n",
    "    rf_prob = rf_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    models['Random Forest'] = rf_model\n",
    "    results['Random Forest'] = {\n",
    "        'accuracy': (rf_pred == y_val).mean(),\n",
    "        'auc': roc_auc_score(y_val, rf_prob),\n",
    "        'predictions': rf_pred,\n",
    "        'probabilities': rf_prob,\n",
    "        'feature_importance': rf_model.feature_importances_\n",
    "    }\n",
    "    \n",
    "    # 3. XGBoost\n",
    "    print(\"3. Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_val)\n",
    "    xgb_prob = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    models['XGBoost'] = xgb_model\n",
    "    results['XGBoost'] = {\n",
    "        'accuracy': (xgb_pred == y_val).mean(),\n",
    "        'auc': roc_auc_score(y_val, xgb_prob),\n",
    "        'predictions': xgb_pred,\n",
    "        'probabilities': xgb_prob,\n",
    "        'feature_importance': xgb_model.feature_importances_\n",
    "    }\n",
    "    \n",
    "    # 4. LightGBM\n",
    "    print(\"4. Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_pred = lgb_model.predict(X_val)\n",
    "    lgb_prob = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    models['LightGBM'] = lgb_model\n",
    "    results['LightGBM'] = {\n",
    "        'accuracy': (lgb_pred == y_val).mean(),\n",
    "        'auc': roc_auc_score(y_val, lgb_prob),\n",
    "        'predictions': lgb_pred,\n",
    "        'probabilities': lgb_prob,\n",
    "        'feature_importance': lgb_model.feature_importances_\n",
    "    }\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "# Train models\n",
    "models, results = train_multiple_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Cell 7: Model Evaluation and Comparison\n",
    "def plot_model_comparison(results, y_val):\n",
    "    \"\"\"Compare model performance\"\"\"\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    for model_name, metrics in results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'AUC': metrics['auc']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    comparison_df.set_index('Model')['Accuracy'].plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Model Accuracy Comparison')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # AUC comparison\n",
    "    comparison_df.set_index('Model')['AUC'].plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title('Model AUC Comparison')\n",
    "    axes[1].set_ylabel('AUC Score')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"ğŸ“Š Model Performance Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "def plot_roc_curves(results, y_val):\n",
    "    \"\"\"Plot ROC curves for all models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        fpr, tpr, _ = roc_curve(y_val, metrics['probabilities'])\n",
    "        auc = metrics['auc']\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(results, y_val):\n",
    "    \"\"\"Plot Precision-Recall curves for all models\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(y_val, metrics['probabilities'])\n",
    "        avg_precision = average_precision_score(y_val, metrics['probabilities'])\n",
    "        plt.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(results, feature_columns):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for model_name, metrics in results.items():\n",
    "        if 'feature_importance' in metrics:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_columns,\n",
    "                'importance': metrics['feature_importance']\n",
    "            }).sort_values('importance', ascending=False).head(15)\n",
    "            \n",
    "            importance_df.set_index('feature')['importance'].plot(kind='barh', ax=axes[plot_idx])\n",
    "            axes[plot_idx].set_title(f'{model_name} - Top 15 Features')\n",
    "            axes[plot_idx].set_xlabel('Importance')\n",
    "            plot_idx += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run evaluations\n",
    "comparison_df = plot_model_comparison(results, y_val)\n",
    "plot_roc_curves(results, y_val)\n",
    "plot_precision_recall_curves(results, y_val)\n",
    "plot_feature_importance(results, feature_columns)\n",
    "\n",
    "# Cell 8: Select Best Model and Final Evaluation\n",
    "# Select best model based on AUC\n",
    "best_model_name = comparison_df.loc[comparison_df['AUC'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"ğŸ† Best model: {best_model_name}\")\n",
    "print(f\"   Validation AUC: {results[best_model_name]['auc']:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_pred = best_model.predict(X_test)\n",
    "test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_prob)\n",
    "\n",
    "print(\"\\nğŸ“Š Final Test Performance:\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "print(f\"Test Accuracy: {(test_pred == y_test).mean():.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Detailed Classification Report:\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Cell 9: Save Model and Metadata\n",
    "def save_model_artifacts(model, feature_columns, model_name, results):\n",
    "    \"\"\"Save model and associated artifacts\"\"\"\n",
    "    \n",
    "    import os\n",
    "    os.makedirs('../models/trained_models', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'../models/trained_models/{model_name.lower().replace(\" \", \"_\")}_fraud_detector.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"âœ… Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_path = '../models/trained_models/feature_names.joblib'\n",
    "    joblib.dump(feature_columns, feature_path)\n",
    "    print(f\"âœ… Features saved to: {feature_path}\")\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        'model_name': f'{model_name} Fraud Detector',\n",
    "        'model_type': model_name,\n",
    "        'version': 'v1.0.0',\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'features': feature_columns,\n",
    "        'n_features': len(feature_columns),\n",
    "        'training_samples': len(X_train),\n",
    "        'validation_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'performance_metrics': {\n",
    "            'validation_auc': float(results[model_name]['auc']),\n",
    "            'validation_accuracy': float(results[model_name]['accuracy']),\n",
    "            'test_auc': float(test_auc),\n",
    "            'test_accuracy': float((test_pred == y_test).mean()),\n",
    "        },\n",
    "        'hyperparameters': str(model.get_params()) if hasattr(model, 'get_params') else 'N/A'\n",
    "    }\n",
    "    \n",
    "    # Add feature importance if available\n",
    "    if 'feature_importance' in results[model_name]:\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': results[model_name]['feature_importance']\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        metadata['feature_importance'] = feature_importance.to_dict('records')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = '../models/trained_models/model_metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Save best model\n",
    "metadata = save_model_artifacts(best_model, feature_columns, best_model_name, results)\n",
    "\n",
    "print(\"\\nğŸ‰ Model training completed successfully!\")\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Final test AUC: {test_auc:.4f}\")\n",
    "print(\"Model artifacts saved to ../models/trained_models/\")\n",
    "\n",
    "# Cell 10: Model Interpretation and Insights\n",
    "def analyze_model_insights(model, feature_columns, X_test, y_test, model_name):\n",
    "    \"\"\"Analyze model insights and feature importance\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ” Analyzing {model_name} insights...\")\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nğŸ“Š Top 10 Most Important Features:\")\n",
    "        print(importance_df.head(10).round(4))\n",
    "        \n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'{model_name} - Feature Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Prediction distribution analysis\n",
    "    test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot probability distributions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.hist(test_prob[y_test == 0], bins=50, alpha=0.7, label='Legitimate', density=True)\n",
    "    plt.hist(test_prob[y_test == 1], bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "    plt.xlabel('Fraud Probability')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Probability Distribution by Class')\n",
    "    plt.legend()\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.subplot(2, 2, 2)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, test_prob)\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, test_prob):.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    plt.subplot(2, 2, 3)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, test_prob)\n",
    "    plt.plot(recall, precision, label=f'PR Curve (AP = {average_precision_score(y_test, test_prob):.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Threshold analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    f1_scores = []\n",
    "    threshold_range = np.arange(0.1, 1.0, 0.05)\n",
    "    for threshold in threshold_range:\n",
    "        pred_thresh = (test_prob > threshold).astype(int)\n",
    "        if len(np.unique(pred_thresh)) > 1:\n",
    "            f1 = 2 * ((pred_thresh * y_test).sum() / pred_thresh.sum()) * ((pred_thresh * y_test).sum() / y_test.sum()) / \\\n",
    "                (((pred_thresh * y_test).sum() / pred_thresh.sum()) + ((pred_thresh * y_test).sum() / y_test.sum()))\n",
    "            f1_scores.append(f1)\n",
    "        else:\n",
    "            f1_scores.append(0)\n",
    "    \n",
    "    plt.plot(threshold_range, f1_scores)\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Optimal threshold\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = threshold_range[optimal_idx]\n",
    "    optimal_f1 = f1_scores[optimal_idx]\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"   F1 score at optimal threshold: {optimal_f1:.3f}\")\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "# Analyze best model\n",
    "optimal_threshold = analyze_model_insights(best_model, feature_columns, X_test, y_test, best_model_name)\n",
    "\n",
    "print(\"\\nâœ… Model training and analysis complete!\")\n",
    "print(\"ğŸ“ All artifacts saved to ../models/trained_models/\")\n",
    "print(\"ğŸš€ Ready for deployment!\")\n",
    "\n",
    "# Cell 11: Summary and Next Steps\n",
    "print(\"\"\"\n",
    "ğŸ‰ FRAUD DETECTION MODEL TRAINING SUMMARY\n",
    "==========================================\n",
    "\n",
    "âœ… Data Processing:\n",
    "   - Loaded and processed training, validation, and test sets\n",
    "   - Created advanced feature engineering pipeline\n",
    "   - Selected optimal feature set\n",
    "\n",
    "âœ… Model Development:\n",
    "   - Trained and compared 4 different models\n",
    "   - Selected best performing model based on AUC\n",
    "   - Achieved optimal performance metrics\n",
    "\n",
    "âœ… Model Evaluation:\n",
    "   - Comprehensive evaluation on test set\n",
    "   - Feature importance analysis\n",
    "   - Optimal threshold determination\n",
    "\n",
    "âœ… Model Artifacts:\n",
    "   - Saved trained model for deployment\n",
    "   - Created comprehensive metadata\n",
    "   - Generated feature importance rankings\n",
    "\n",
    "ğŸš€ NEXT STEPS:\n",
    "   1. Deploy model to production API\n",
    "   2. Set up monitoring and drift detection\n",
    "   3. Implement feedback loops for continuous learning\n",
    "   4. A/B test against existing systems\n",
    "\n",
    "ğŸ“Š KEY PERFORMANCE METRICS:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"   Model Type: {best_model_name}\")\n",
    "print(f\"   Test AUC: {test_auc:.4f}\")\n",
    "print(f\"   Test Accuracy: {(test_pred == y_test).mean():.4f}\")\n",
    "print(f\"   Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "print(f\"   Features Used: {len(feature_columns)}\")\n",
    "print(\"   Training Time: ~5-10 minutes\")\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ”— INTEGRATION READY:\n",
    "   - Model files: ../models/trained_models/\n",
    "   - API Integration: Load model with joblib\n",
    "   - Feature Pipeline: Use create_advanced_features()\n",
    "   - Prediction: model.predict_proba(X)[:, 1]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
